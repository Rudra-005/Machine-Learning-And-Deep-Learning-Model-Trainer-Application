# ML/DL Trainer

**A production-ready web platform for training, evaluating, and deploying machine learning and deep learning models.**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status: Production Ready](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)](#)

## Overview

ML/DL Trainer is an end-to-end machine learning platform that simplifies the model development lifecycle. Upload data, configure hyperparameters, train models, and download results‚Äîall through an intuitive web interface. Supports 9 ML algorithms and 3 DL architectures with automatic preprocessing, cross-validation, and comprehensive evaluation metrics.

## ‚ú® Key Features

| Feature | Details |
|---------|---------|
| **üì§ Data Upload** | CSV file upload with automatic validation and quality checks |
| **üîç EDA** | Exploratory data analysis with missing value detection, feature relationships, and target analysis |
| **üéØ Model Selection** | 9 ML algorithms (Scikit-learn) + 3 DL architectures (TensorFlow/Keras) |
| **‚öôÔ∏è Hyperparameter Tuning** | Per-model configuration for learning rate, epochs, batch size, tree depth, etc. |
| **üîÑ Preprocessing** | Automatic missing value imputation, feature scaling, categorical encoding |
| **üìä Evaluation** | Classification & regression metrics, confusion matrices, feature importance plots |
| **üíæ Model Persistence** | Download trained models (PKL) and metrics (JSON) |
| **üöÄ Production Ready** | Error handling, logging, memory monitoring, Docker support |

## ü§ñ Supported Models

### Machine Learning (Scikit-learn)
- **Classification**: Logistic Regression, Random Forest, SVM, KNN, Gradient Boosting
- **Regression**: Linear Regression, Random Forest, SVR, Gradient Boosting

### Deep Learning (TensorFlow/Keras)
- Sequential Neural Networks
- Convolutional Neural Networks (CNN)
- Recurrent Neural Networks (LSTM)

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Streamlit Frontend (Port 8501)  ‚îÇ
‚îÇ  (Upload, Config, Training, Results)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                  ‚îÇ              ‚îÇ          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇPreprocessing‚îÇ   ‚îÇModel    ‚îÇ   ‚îÇEval.  ‚îÇ   ‚îÇStorage ‚îÇ
‚îÇ & Features  ‚îÇ   ‚îÇTraining ‚îÇ   ‚îÇMetrics‚îÇ   ‚îÇ(PKL)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Project Structure

```
ML_DL_Trainer/
‚îú‚îÄ‚îÄ app/                          # Frontend (Streamlit)
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eda_page.py          # EDA visualization
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ error_handler.py      # Error handling & logging
‚îÇ       ‚îú‚îÄ‚îÄ file_handler.py       # File operations
‚îÇ       ‚îú‚îÄ‚îÄ logger.py             # Logging setup
‚îÇ       ‚îî‚îÄ‚îÄ validators.py         # Data validation
‚îú‚îÄ‚îÄ core/                         # ML operations
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py           # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineer.py       # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ target_analyzer.py        # Target analysis
‚îÇ   ‚îî‚îÄ‚îÄ validator.py              # Data validation
‚îú‚îÄ‚îÄ models/                       # Model implementations
‚îÇ   ‚îú‚îÄ‚îÄ model_factory.py          # Factory pattern
‚îÇ   ‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py         # ML classifiers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ regressor.py          # ML regressors
‚îÇ   ‚îî‚îÄ‚îÄ dl/
‚îÇ       ‚îú‚îÄ‚îÄ cnn_models.py         # CNN architectures
‚îÇ       ‚îî‚îÄ‚îÄ rnn_models.py         # RNN architectures
‚îú‚îÄ‚îÄ evaluation/                   # Evaluation utilities
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                # Metrics calculation
‚îÇ   ‚îú‚îÄ‚îÄ visualizer.py             # Plotting
‚îÇ   ‚îú‚îÄ‚îÄ cross_validator.py        # Cross-validation
‚îÇ   ‚îî‚îÄ‚îÄ reporter.py               # Report generation
‚îú‚îÄ‚îÄ storage/                      # Data persistence
‚îÇ   ‚îú‚îÄ‚îÄ model_repository.py       # Model storage
‚îÇ   ‚îú‚îÄ‚îÄ result_repository.py      # Results storage
‚îÇ   ‚îî‚îÄ‚îÄ cache_manager.py          # Caching
‚îú‚îÄ‚îÄ data/                         # Data directories
‚îÇ   ‚îú‚îÄ‚îÄ uploads/                  # User uploads
‚îÇ   ‚îú‚îÄ‚îÄ preprocessed/             # Processed data
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Trained models
‚îÇ   ‚îî‚îÄ‚îÄ results/                  # Experiment results
‚îú‚îÄ‚îÄ tests/                        # Unit tests
‚îú‚îÄ‚îÄ Dockerfile                    # Container image
‚îú‚îÄ‚îÄ docker-compose.yml            # Multi-container setup
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ README.md                     # This file
```

## üöÄ Quick Start

### Prerequisites
- Python 3.11+
- Docker (optional)
- 2GB RAM minimum

### Local Installation

1. **Clone repository**
   ```bash
   git clone https://github.com/yourusername/ML_DL_Trainer.git
   cd ML_DL_Trainer
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run application**
   ```bash
   streamlit run app/main.py
   ```

   Application opens at `http://localhost:8501`

### Docker Deployment

1. **Build image**
   ```bash
   docker build -t ml-dl-trainer:latest .
   ```

2. **Run container**
   ```bash
   docker run -p 8501:8501 \
     -v $(pwd)/data:/app/data \
     ml-dl-trainer:latest
   ```

3. **Using Docker Compose**
   ```bash
   docker-compose up -d
   ```

## üìñ Usage Workflow

### Step 1: Upload Data
- Navigate to **Data Upload** page
- Upload CSV file or load sample dataset (Iris, Wine)
- Review data preview, statistics, and column info

### Step 2: Explore Data (Optional)
- Go to **EDA / Data Understanding** page
- Analyze missing values, feature distributions, relationships
- Identify target variable characteristics

### Step 3: Configure & Train
- Select **Training** page
- Choose task type: Classification or Regression
- Select algorithm and set hyperparameters
- Click **Start Training**

### Step 4: Review Results
- View performance metrics on **Results** page
- Download trained model (PKL format)
- Export metrics (JSON format)

## ‚öôÔ∏è Configuration

Edit `app/config.py` to customize:

```python
MAX_FILE_SIZE = 500 * 1024 * 1024  # Max upload: 500MB
DEFAULT_TEST_SIZE = 0.2             # Train-test split
DEFAULT_CV_FOLDS = 5                # Cross-validation folds
DEFAULT_EPOCHS = 50                 # DL epochs
DEFAULT_BATCH_SIZE = 32             # DL batch size
LOG_LEVEL = "INFO"                  # Logging level
```

## üß™ Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_core.py -v

# Run with coverage
pytest tests/ --cov=core --cov=models
```

## üìä Screenshots

### Home Page
- Platform overview with feature highlights
- Quick start guide with 3-step workflow
- Supported models showcase
- Call-to-action buttons

### Data Upload
- Drag-and-drop CSV upload
- Sample dataset loading (Iris, Wine)
- Data preview with statistics
- Column information display

### EDA / Data Understanding
- Missing value analysis
- Feature distribution plots
- Correlation heatmaps
- Target variable analysis
- Relationship visualization

### Training
- Task type selection (Classification/Regression)
- Algorithm selection with model-specific hyperparameters
- Real-time training progress
- Target validation with warnings

### Results
- Performance metrics display
- Model download (PKL)
- Metrics export (JSON)
- Detailed evaluation results

### About
- Platform information
- Supported algorithms list
- Architecture overview
- Quick links and acknowledgments

## üîí Security & Production Features

- ‚úÖ Input validation for file uploads
- ‚úÖ Error handling with custom exceptions
- ‚úÖ Memory monitoring (90% threshold)
- ‚úÖ Comprehensive logging with file rotation
- ‚úÖ Non-root Docker user
- ‚úÖ Health checks in container
- ‚úÖ Environment-based configuration
- ‚ö†Ô∏è TODO: User authentication
- ‚ö†Ô∏è TODO: Role-based access control

## üìà Performance Optimization

| Optimization | Implementation |
|--------------|-----------------|
| **Caching** | @st.cache_data with TTL for expensive computations |
| **Sampling** | 10% sampling for datasets >100K rows in visualizations |
| **Preprocessing** | Vectorized operations with NumPy/Pandas |
| **Memory** | MemoryMonitor tracks usage, prevents OOM |
| **Logging** | Async logging to avoid blocking UI |

## üêõ Troubleshooting

| Issue | Solution |
|-------|----------|
| Memory error with large datasets | Increase system RAM or use data streaming |
| Slow training | Reduce batch size or feature count |
| Import errors | Run `pip install -r requirements.txt` |
| Port 8501 already in use | `streamlit run app/main.py --server.port 8502` |
| Docker build fails | Ensure Docker daemon is running |

## üì¶ Deployment Options

### AWS
```bash
# EC2 + S3 + RDS
- EC2 for app hosting
- S3 for model/data storage
- RDS for metadata database
```

### Google Cloud
```bash
# Cloud Run + Cloud Storage
- Cloud Run for serverless deployment
- Cloud Storage for models
- Cloud SQL for database
```

### Azure
```bash
# App Service + Blob Storage
- App Service for hosting
- Blob Storage for models
- SQL Database for metadata
```

### Heroku
```bash
git push heroku main
```

## ü§ù Contributing

1. Fork repository
2. Create feature branch: `git checkout -b feature/amazing`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing`
5. Create Pull Request

## üìÑ License

MIT License - see [LICENSE](LICENSE) file

## üìû Support

- üìß Email: support@example.com
- üêõ Issues: [GitHub Issues](https://github.com/yourusername/ML_DL_Trainer/issues)
- üìñ Docs: [Wiki](https://github.com/yourusername/ML_DL_Trainer/wiki)

## üó∫Ô∏è Roadmap

- [ ] User authentication & RBAC
- [ ] Model versioning & registry
- [ ] Hyperparameter optimization (Optuna)
- [ ] AutoML integration
- [ ] Model explainability (SHAP, LIME)
- [ ] Real-time collaboration
- [ ] Mobile app
- [ ] Advanced visualizations (Plotly)
- [ ] API endpoint documentation
- [ ] Performance benchmarking

## üìä Resume-Ready Project Explanation

**ML/DL Trainer** is a full-stack machine learning platform demonstrating end-to-end software engineering practices:

### Technical Stack
- **Frontend**: Streamlit (Python web framework)
- **Backend**: FastAPI, Python
- **ML/DL**: Scikit-learn (9 algorithms), TensorFlow/Keras (3 architectures)
- **Data**: Pandas, NumPy
- **DevOps**: Docker, Docker Compose
- **Testing**: Pytest

### Key Accomplishments
1. **Architecture**: Implemented factory pattern for extensible model creation, repository pattern for data persistence
2. **Data Pipeline**: Built preprocessing pipeline with automatic missing value handling, feature scaling, categorical encoding
3. **Error Handling**: Developed comprehensive error handling module with custom exceptions, memory monitoring, production logging
4. **UI/UX**: Created intuitive Streamlit interface with 6 pages, real-time feedback, sample datasets
5. **Production Ready**: Added Docker support, health checks, non-root user, environment configuration
6. **Testing**: Wrote unit tests for core components (feature analysis, target detection, preprocessing)

### Design Patterns Used
- **Factory Pattern**: ModelFactory for flexible model creation
- **Repository Pattern**: Model and result storage abstraction
- **Pipeline Pattern**: Data preprocessing pipeline
- **Observer Pattern**: Real-time training callbacks
- **Decorator Pattern**: Error handling decorators

### Scalability Features
- Caching with TTL for expensive computations
- Data sampling for large datasets
- Memory monitoring to prevent OOM
- Async logging
- Containerization for cloud deployment

